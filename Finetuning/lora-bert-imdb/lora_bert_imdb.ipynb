{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Srujan Landeri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Srujan Landeri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n",
    "import evaluate\n",
    "import torch\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# model checkpoint\n",
    "checkpoint = \"distilbert-base-uncased\"\n",
    "\n",
    "# label maps\n",
    "id2label = {0: \"Negative\", 1: \"Positive\"}\n",
    "label2id = {\"Negative\": 0, \"Positive\": 1}\n",
    "\n",
    "\"\"\"\n",
    "including id2label and label2id mappings in sentiment analysis models isn't strictly\n",
    "necessary, but it can be highly beneficial, especially when working with multi-class \n",
    "or complex datasets. \n",
    "\"\"\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "dataset = load_dataset(\"shawhin/imdb-truncated\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['validation']['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \n",
    "    text = examples[\"text\"] # extract all texts\n",
    "\n",
    "    # tokenize the texts\n",
    "    tokenizer.truncation_side = 'left'  # if length exceeds max_length, truncate from left\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"np\", # return numpy arrays\n",
    "        truncation=True,     # truncate the texts\n",
    "        max_length=512,      # max length of the tokenized inputs\n",
    "    )\n",
    "\n",
    "    return tokenized_inputs\n",
    "\n",
    "# adding pad token to the tokenized inputs\n",
    "if tokenizer.pad_token is None:\n",
    "    pad_token = '[PAD]'\n",
    "    tokenizer.add_special_tokens({'pad_token': pad_token})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    \"\"\"\n",
    "    After adding the padding token to the tokenizer’s vocabulary, this line \n",
    "    updates the model’s embeddings to account for the new token. \n",
    "    resize_token_embeddings(len(tokenizer)) resizes the embedding layer \n",
    "    to match the updated tokenizer vocabulary size, ensuring the model \n",
    "    can recognize and correctly process the new padding token.\n",
    "    \"\"\"\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load('accuracy')\n",
    "\n",
    "def compute_metrics(logits):\n",
    "    predictions, labels = logits\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained Model Predictions:\n",
      "--------------------------------------------------\n",
      "Text: It was good. : Positive\n",
      "--------------------------------------------------\n",
      "Text: Not a fan, don't recommend. : Positive\n",
      "--------------------------------------------------\n",
      "Text: Better than the first one. : Positive\n",
      "--------------------------------------------------\n",
      "Text: Not worth watching. : Positive\n",
      "--------------------------------------------------\n",
      "Text: This one is a pass. : Positive\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# evaluating Base Model\n",
    "\n",
    "input_list = [\n",
    "    \"It was good.\",\n",
    "    \"Not a fan, don't recommend.\",\n",
    "    \"Better than the first one.\",\n",
    "    \"Not worth watching.\",\n",
    "    \"This one is a pass.\",\n",
    "]\n",
    "\n",
    "print(\"Untrained Model Predictions:\")\n",
    "print(\"-\"*50)\n",
    "for text in input_list:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    predicted_class = torch.argmax(outputs.logits)\n",
    "    \n",
    "    print(f\"Text: {text} : {id2label[predicted_class.item()]}\")\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 628,994 || all params: 67,584,004 || trainable%: 0.9307\n"
     ]
    }
   ],
   "source": [
    "# lora config\n",
    "peft_config = LoraConfig(\n",
    "    task_type=\"SEQ_CLS\",        # Sequence Classification Task\n",
    "    r=4,                        # Intrinsic rank of the model\n",
    "    lora_alpha=32,              # Alpha value for Lora\n",
    "    lora_dropout=0.01,          # Dropout rate for Lora\n",
    "    target_modules=[\"q_lin\"]    # Apply Lora to the query linear layer\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config=peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "lr = 1e-3\n",
    "batch_size = 4\n",
    "num_epochs = 10\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    learning_rate=lr,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 10%|█         | 250/2500 [00:41<04:14,  8.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4334530532360077, 'eval_accuracy': 0.869, 'eval_runtime': 13.0503, 'eval_samples_per_second': 76.626, 'eval_steps_per_second': 19.157, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 500/2500 [01:11<03:31,  9.46it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4337, 'grad_norm': 20.269641876220703, 'learning_rate': 0.0008, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 20%|██        | 500/2500 [01:25<03:31,  9.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.42678600549697876, 'eval_accuracy': 0.871, 'eval_runtime': 13.1504, 'eval_samples_per_second': 76.043, 'eval_steps_per_second': 19.011, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 30%|███       | 750/2500 [02:08<03:37,  8.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5711933374404907, 'eval_accuracy': 0.874, 'eval_runtime': 13.0831, 'eval_samples_per_second': 76.435, 'eval_steps_per_second': 19.109, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 1000/2500 [02:40<03:08,  7.94it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.21, 'grad_norm': 0.14531736075878143, 'learning_rate': 0.0006, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 40%|████      | 1000/2500 [02:53<03:08,  7.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.670844316482544, 'eval_accuracy': 0.877, 'eval_runtime': 13.2922, 'eval_samples_per_second': 75.232, 'eval_steps_per_second': 18.808, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 50%|█████     | 1250/2500 [03:36<02:22,  8.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8383438587188721, 'eval_accuracy': 0.872, 'eval_runtime': 12.9862, 'eval_samples_per_second': 77.005, 'eval_steps_per_second': 19.251, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 1500/2500 [04:06<02:06,  7.92it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0599, 'grad_norm': 0.00040850063669495285, 'learning_rate': 0.0004, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 60%|██████    | 1500/2500 [04:19<02:06,  7.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9231562614440918, 'eval_accuracy': 0.871, 'eval_runtime': 13.0635, 'eval_samples_per_second': 76.549, 'eval_steps_per_second': 19.137, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 70%|███████   | 1750/2500 [05:01<01:38,  7.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9480758309364319, 'eval_accuracy': 0.881, 'eval_runtime': 13.0089, 'eval_samples_per_second': 76.871, 'eval_steps_per_second': 19.218, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 2000/2500 [05:31<01:03,  7.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.028, 'grad_norm': 0.006741494871675968, 'learning_rate': 0.0002, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 80%|████████  | 2000/2500 [05:44<01:03,  7.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9871799349784851, 'eval_accuracy': 0.877, 'eval_runtime': 13.0067, 'eval_samples_per_second': 76.884, 'eval_steps_per_second': 19.221, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 90%|█████████ | 2250/2500 [06:27<00:29,  8.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0265973806381226, 'eval_accuracy': 0.873, 'eval_runtime': 12.9771, 'eval_samples_per_second': 77.059, 'eval_steps_per_second': 19.265, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [06:57<00:00, 10.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0068, 'grad_norm': 0.011574480682611465, 'learning_rate': 0.0, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|██████████| 2500/2500 [07:10<00:00, 10.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0210520029067993, 'eval_accuracy': 0.879, 'eval_runtime': 13.0334, 'eval_samples_per_second': 76.726, 'eval_steps_per_second': 19.181, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [07:11<00:00,  5.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 431.4919, 'train_samples_per_second': 23.175, 'train_steps_per_second': 5.794, 'train_loss': 0.1476790029525757, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2500, training_loss=0.1476790029525757, metrics={'train_runtime': 431.4919, 'train_samples_per_second': 23.175, 'train_steps_per_second': 5.794, 'total_flos': 1113026652407424.0, 'train_loss': 0.1476790029525757, 'epoch': 10.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Model Predictions:\n",
      "--------------------------------------------------\n",
      "Text: It was good. : Positive\n",
      "--------------------------------------------------\n",
      "Text: Not a fan, don't recommend. : Positive\n",
      "--------------------------------------------------\n",
      "Text: Better than the first one. : Positive\n",
      "--------------------------------------------------\n",
      "Text: Not worth watching. : Negative\n",
      "--------------------------------------------------\n",
      "Text: This one is a pass. : Positive\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# evaluating Finetuned Model\n",
    "\n",
    "input_list = [\n",
    "    \"It was good.\",\n",
    "    \"Not a fan, don't recommend.\",\n",
    "    \"Better than the first one.\",\n",
    "    \"Not worth watching.\",\n",
    "    \"This one is a pass.\",\n",
    "]\n",
    "\n",
    "print(\"Trained Model Predictions:\")\n",
    "print(\"-\"*50)\n",
    "for text in input_list:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "    outputs = model(**inputs)\n",
    "    predicted_class = torch.argmax(outputs.logits)\n",
    "    \n",
    "    print(f\"Text: {text} : {id2label[predicted_class.item()]}\")\n",
    "    print(\"-\"*50)\n",
    "\n",
    "\"\"\"\n",
    "Results are better than the untrained model, but the model still struggles...\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
