{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "tokens = tokenizer.tokenize(\"Hello, my dog is cute\")\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids = torch.tensor(input_ids) \n",
    "\n",
    "model(input_ids) # error -> model expects a tensor while we are passing a list of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-4.0687,  4.3669]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\").input_ids\n",
    "model(input_ids) # works fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.7674,  1.9746]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids = torch.tensor([input_ids]) # added a new dimension to the input_ids list\n",
    "output = model(input_ids) # works fine\n",
    "\n",
    "print(output.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Batching is the act of sending multiple sentences through the model, all at once. If you only have one sentence, you can just build a batch with a single sequence.._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "sequence = \"Hello, my dog is cute\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.7674,  1.9746],\n",
      "        [-1.7674,  1.9746]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batched_ids = [\n",
    "    ids,\n",
    "    ids\n",
    "]\n",
    "\n",
    "batched_input_ids = torch.tensor(batched_ids)\n",
    "output = model(batched_input_ids)\n",
    "\n",
    "print(output.logits)    # same as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['casual', 'casual', 'casual']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# padding in batched input\n",
    "\n",
    "\"\"\"\n",
    "tensors accept sequences of the same length. If the sequences are of different lengths, we can pad the sequences to make them of the same length.\n",
    "\"\"\"\n",
    "\n",
    "batched_ids = [\n",
    "    [10017, 10017, 10017],\n",
    "    [10017, 10017],\n",
    "] # different lengths\n",
    "\n",
    "PADDING_TOKEN_ID = 100\n",
    "batched_ids = [\n",
    "    [10017, 10017, 10017],\n",
    "    [10017, 10017, PADDING_TOKEN_ID]\n",
    "] # same lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3784, -0.3921]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.2837, -0.1771]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.3784, -0.3921],\n",
      "        [ 0.3961, -0.4254]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "sequence1_ids = [[200, 200, 200]]\n",
    "sequence2_ids = [[200, 200]]\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id], # padding_token of that tokenizer\n",
    "]\n",
    "\n",
    "print(model(torch.tensor(sequence1_ids)).logits)\n",
    "print(model(torch.tensor(sequence2_ids)).logits)\n",
    "print(model(torch.tensor(batched_ids)).logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There’s something wrong with the logits in our batched predictions: the second row should be the same as the logits \n",
    "for the second sentence, but we’ve got completely different values! This is because the key feature of Transformer \n",
    "models is attention layers that contextualize each token. These will take into account the padding tokens since \n",
    "they attend to all of the tokens of a sequence. To get the same result when passing individual sentences of \n",
    "different lengths through the model or when passing a batch with the same sentences and padding applied, \n",
    "\n",
    "__we need to tell those attention layers to ignore the padding tokens. This is done by using an attention mask.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3784, -0.3921]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.2837, -0.1771]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.3784, -0.3921],\n",
      "        [ 0.2837, -0.1771]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "attention_mask = [\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 0],  # 0 means ignore this token\n",
    "]\n",
    "\n",
    "\n",
    "print(model(torch.tensor(sequence1_ids)).logits)\n",
    "print(model(torch.tensor(sequence2_ids)).logits)\n",
    "outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\n",
    "print(outputs.logits)\n",
    "\n",
    "\"\"\"\n",
    "Batched logits now match with the sequence1_ids and sequence2_ids logits\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
