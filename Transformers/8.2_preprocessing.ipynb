{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_dataset = load_dataset(\"glue\", \"mrpc\")\n",
    "\n",
    "train_dataset = raw_dataset[\"train\"]\n",
    "validation_dataset = raw_dataset[\"validation\"]\n",
    "test_dataset = raw_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: [[101, 2572, 3217, 5831, 5496, 2010, 2567, 1010, 3183, 2002, 2170, 1000, 1996, 7409, 1000, 1010, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102, 7727, 2000, 2032, 2004, 2069, 1000, 1996, 7409, 1000, 1010, 2572, 3217, 5831, 5496, 2010, 2567, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102]]\n",
      "token_type_ids: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "attention_mask: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "sentence_1 = train_dataset[0][\"sentence1\"]\n",
    "sentence_2 = train_dataset[0][\"sentence2\"]\n",
    "\n",
    "# For checking if two senteces are similar or not\n",
    "# we have to pass the two sentences to the tokenizer as a pair\n",
    "\n",
    "inputs = tokenizer(sentence_1, sentence_2, return_tensors=\"pt\")\n",
    "\n",
    "for key, value in inputs.items():\n",
    "    print(f\"{key}: {value.numpy().tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'am', '##ro', '##zi', 'accused', 'his', 'brother', ',', 'whom', 'he', 'called', '\"', 'the', 'witness', '\"', ',', 'of', 'deliberately', 'di', '##stor', '##ting', 'his', 'evidence', '.', '[SEP]', 'referring', 'to', 'him', 'as', 'only', '\"', 'the', 'witness', '\"', ',', 'am', '##ro', '##zi', 'accused', 'his', 'brother', 'of', 'deliberately', 'di', '##stor', '##ting', 'his', 'evidence', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "ids = inputs[\"input_ids\"]\n",
    "print(tokenizer.convert_ids_to_tokens(ids.numpy().tolist()[0]))\n",
    "\n",
    "# as we observe the pattern here is in the form of `[CLS] sentence1 [SEP] sentence2 [SEP]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenizer(\n",
    "    raw_dataset[\"train\"][\"sentence1\"],\n",
    "    raw_dataset[\"train\"][\"sentence2\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works well, but it has the disadvantage of returning a dictionary (with our keys, input_ids, attention_mask, and token_type_ids, and values that are lists of lists).\n",
    "It will also only work if you have enough RAM to store your whole dataset during the tokenization (whereas the datasets from the Hugginface Datasets library are Apache Arrow files\n",
    "stored on the disk, so you only keep the samples you ask for loaded in memory).\n",
    "\n",
    "### 1. **Tokenization Returns a Dictionary:**\n",
    "When you use the `tokenizer` to process the dataset, it returns a dictionary. For example, when you tokenize sentences with a Hugging Face tokenizer:\n",
    "\n",
    "```python\n",
    "tokenized_dataset = tokenizer(\n",
    "    raw_dataset[\"train\"][\"sentence1\"],\n",
    "    raw_dataset[\"train\"][\"sentence2\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    ")\n",
    "```\n",
    "\n",
    "The result (`tokenized_dataset`) is a dictionary containing three keys:\n",
    "- `input_ids`: The tokenized representations (IDs) of the words.\n",
    "- `attention_mask`: A mask indicating which tokens should be attended to (1 for tokens, 0 for padding).\n",
    "- `token_type_ids`: Used to distinguish between the two sentences in a pair (for models like BERT).\n",
    "\n",
    "Each key contains a **list of lists**, where:\n",
    "- Each **list** corresponds to one tokenized sentence or sentence pair.\n",
    "- So, if you're tokenizing many sentences, youâ€™re essentially storing all this information as a large dictionary, which can take up significant memory.\n",
    "\n",
    "### 2. **Memory Concerns:**\n",
    "When you tokenize the dataset this way, you end up storing all tokenized data in **RAM (random access memory)**. For small datasets, this is manageable. However, for large datasets, this approach can quickly fill up your system's RAM because:\n",
    "- Each tokenized sentence is stored as a list of integers (`input_ids`, `attention_mask`, etc.).\n",
    "- For a large dataset with thousands or millions of samples, this can become inefficient, as you are storing all the tokenized sentences in memory at once.\n",
    "\n",
    "### 3. **ðŸ¤— Datasets Library and Apache Arrow:**\n",
    "The **ðŸ¤— Datasets library** solves this memory problem by using **Apache Arrow**, a columnar storage format that is optimized for efficient reading/writing of large datasets, while keeping memory usage low.\n",
    "\n",
    "Hereâ€™s how the ðŸ¤— Datasets library works with Arrow files:\n",
    "- **Disk-backed storage**: Instead of keeping the whole dataset in memory, the data is stored on disk in the Arrow format. Only the samples you are working on at any given moment are loaded into memory.\n",
    "- **Efficient access**: You can retrieve samples as you need them without having to load the entire dataset into memory. This is particularly useful when working with large datasets like those for NLP tasks (e.g., millions of text samples).\n",
    "\n",
    "### 4. **The Disadvantage of Tokenizing Everything at Once:**\n",
    "When you tokenize the entire dataset at once (as in the code you provided), you lose the benefits of disk-backed datasets:\n",
    "- **RAM limitations**: The whole tokenized dataset must fit in memory. If your dataset is large, you may run out of memory, causing performance issues or crashes.\n",
    "- **No incremental loading**: All samples are tokenized and stored at once, even though you might not need all of them in memory simultaneously.\n",
    "\n",
    "### 5. **Alternatives:**\n",
    "Instead of tokenizing everything at once, you can use the **`datasets.map()`** function, which tokenizes the dataset in a more efficient, disk-backed manner:\n",
    "\n",
    "```python\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], padding=True, truncation=True)\n",
    "\n",
    "# Apply tokenization to the dataset using map, without loading everything into memory\n",
    "tokenized_dataset = raw_dataset.map(tokenize_function, batched=True)\n",
    "```\n",
    "\n",
    "### Advantages of Using `datasets.map()`:\n",
    "- The `map()` function applies the tokenization to the dataset **on-the-fly**, meaning it processes batches of samples and updates the dataset without storing everything in memory.\n",
    "- It leverages **Apache Arrow** to keep most of the dataset on disk, only loading and tokenizing batches as needed.\n",
    "- This approach is far more scalable for large datasets, as you donâ€™t need to worry about memory limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3668/3668 [00:00<00:00, 10015.57 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 408/408 [00:00<00:00, 9556.20 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1725/1725 [00:00<00:00, 11845.29 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Dataset:  Dataset({\n",
      "    features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      "    num_rows: 3668\n",
      "})\n",
      "Actual Dataset:  Dataset({\n",
      "    features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      "    num_rows: 408\n",
      "})\n",
      "Actual Dataset:  Dataset({\n",
      "    features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      "    num_rows: 1725\n",
      "})\n",
      "\n",
      "****************************************************************************************************\n",
      "\n",
      "Tokenized Dataset:  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 3668\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 408\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 1725\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = raw_dataset.map(tokenize_function, batched=True) # optionally you can use num_proc=4 to use multiple cores\n",
    "\n",
    "print(\"Actual Dataset: \", train_dataset)\n",
    "print(\"Actual Dataset: \", validation_dataset)\n",
    "print(\"Actual Dataset: \", test_dataset)\n",
    "\n",
    "print()\n",
    "print(\"*\" * 100)\n",
    "print()\n",
    "\n",
    "print(\"Tokenized Dataset: \", tokenized_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization Process Breakdown:\n",
    "Tokenization = Splitting into Tokens + Converting to IDs:\n",
    "\n",
    "Splitting into tokens: The tokenizer breaks each sentence into subword tokens, based on the tokenizerâ€™s vocabulary. For example, the sentence \"I love machine learning\" could be split into the tokens ['I', 'love', 'machine', 'learning'].\n",
    "Converting tokens to input_ids: Each token is then mapped to a unique integer ID in the tokenizer's vocabulary. For example, ['I', 'love', 'machine', 'learning'] might be mapped to [101, 2307, 3347, 6754] (actual IDs may vary depending on the tokenizer's vocabulary).\n",
    "input_ids: This is the main output of tokenization. It represents the tokenized form of the sentences, where each word or subword is replaced by its corresponding integer ID from the tokenizerâ€™s vocabulary.\n",
    "\n",
    "token_type_ids (specific to BERT):\n",
    "\n",
    "Since you are using BERT, which supports input pairs (sentence pairs for tasks like classification), token_type_ids are added to distinguish between the two input sentences.\n",
    "For BERT:\n",
    "Sentence 1 is marked with 0 for all its tokens.\n",
    "Sentence 2 is marked with 1 for all its tokens.\n",
    "This helps the model understand which tokens belong to which sentence.\n",
    "attention_mask (applies to all tokenizers):\n",
    "\n",
    "The attention mask is used to specify which tokens should be attended to by the model and which are just padding tokens (to ensure all inputs are the same length).\n",
    "It is a sequence of 1s and 0s, where:\n",
    "1 indicates that the token should be attended to (itâ€™s part of the original sentence).\n",
    "0 indicates that the token is just padding and should be ignored.\n",
    "Regardless of the tokenizer you use, the attention mask is applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DYNAMIC PADDING__:\n",
    "\n",
    "It is often required to have all the tensors be padded to same length, but depending on the database the sentences can actually be of varied length \n",
    "\n",
    "\n",
    "suppose length of the shortest sentence is around 20 tokens\n",
    "and length of the largest sentence is around 200 tokens\n",
    "then each of the sentence has to padded to 200 tokens leading to wastage of memory and computation\n",
    "hence we have to use dynamic padding, that pads the sentences based on the sentences in the batch itself\n",
    "not considering all the sentences in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 226\n",
      "42 215\n"
     ]
    }
   ],
   "source": [
    "sentences_pair_1 = [len(sentence['sentence1']) for sentence in train_dataset]\n",
    "sentences_pair_2 = [len(sentence['sentence2']) for sentence in train_dataset]\n",
    "\n",
    "print(min(sentences_pair_1), max(sentences_pair_1))\n",
    "print(min(sentences_pair_2), max(sentences_pair_2))\n",
    "\n",
    "# here the min length is 38 and max is 226\n",
    "# here the min length is 42 and max is 215\n",
    "\n",
    "# if all sentences are processed together then the padding will be done to the max length of the sentence\n",
    "# which will be a waste of memory, unessary padding will be added to the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "59\n",
      "47\n",
      "67\n",
      "59\n",
      "50\n",
      "62\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "samples = tokenized_dataset[\"train\"][:8]\n",
    "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n",
    "\n",
    "for i in samples['input_ids']:\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': torch.Size([8, 67]), 'token_type_ids': torch.Size([8, 67]), 'attention_mask': torch.Size([8, 67]), 'labels': torch.Size([8])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "batch = data_collator(samples)\n",
    "print({k: v.shape for k, v in batch.items()})   # all the sentences are padded to the max length of the sentence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
